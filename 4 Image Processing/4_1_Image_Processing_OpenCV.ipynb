{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision"
      ],
      "metadata": {
        "id": "Y_YEwg2sFQBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images as Numerical Data\n",
        "\n",
        "An image can be represented as a 2D function F(x,y) where x and y are spatial coordinates. The amplitude of F at a particular value of x,y is known as the intensity of an image at that point. If x,y, and the amplitude value is finite then we call it a digital image.\n",
        "\n",
        "It is an array of pixels arranged in columns and rows. Pixels are the elements of an image that contain information about intensity and color. An image can also be represented in 3D where x,y, and z become spatial coordinates. Pixels are arranged in the form of a matrix. This is known as an **RGB image**."
      ],
      "metadata": {
        "id": "gMdsaX6OM7o2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCthPyGDFHxV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.image as mpimg  # for reading in images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2  # computer vision library\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the image\n",
        "image = mpimg.imread('img_url')\n",
        "\n",
        "# Print out the image dimensions\n",
        "print('Image dimensions:', image.shape)\n",
        "\n",
        "# Change from color to grayscale\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "plt.imshow(gray_image, cmap='gray')"
      ],
      "metadata": {
        "id": "Rn6LHBmNlB-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print specific grayscale pixel values\n",
        "# What is the pixel value at x = 400 and y = 300 (on the body of the car)?\n",
        "# format is : [y,x]\n",
        "print(gray_image[300,400])"
      ],
      "metadata": {
        "id": "KZ3AfwaLlQmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the maximum and minimum grayscale values in this image\n",
        "max_val = np.amax(gray_image)\n",
        "min_val = np.amin(gray_image)\n",
        "\n",
        "print('Max: ', max_val)\n",
        "print('Min: ', min_val)"
      ],
      "metadata": {
        "id": "pF3yU5iwlUyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 5x5 image using just grayscale, numerical values\n",
        "tiny_image = np.array([[0, 20, 30, 150, 120],\n",
        "                      [200, 200, 250, 70, 3],\n",
        "                      [50, 180, 85, 40, 90],\n",
        "                      [240, 100, 50, 255, 10],\n",
        "                      [30, 0, 75, 190, 220]])\n",
        "\n",
        "# To show the pixel grid, use matshow\n",
        "plt.matshow(tiny_image, cmap='gray')\n",
        "\n",
        "## TODO: See if you can draw a tiny smiley face or something else!"
      ],
      "metadata": {
        "id": "tHdrLfJhlfva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize RGB Colorspaces"
      ],
      "metadata": {
        "id": "zeZg7TY4lnAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "bg-kohLCltJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the image\n",
        "image = mpimg.imread('image_url')\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "THzpSAKflv0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blue_image = image[:,:,0]\n",
        "cv2_imshow(blue_image)"
      ],
      "metadata": {
        "id": "ePxIvExblz4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "green_image = image[:,:,1]\n",
        "cv2_imshow(green_image)"
      ],
      "metadata": {
        "id": "eekMzg7xme35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "red_image = image[:,:,2]\n",
        "cv2_imshow(red_image)"
      ],
      "metadata": {
        "id": "tfWDXpD6mh08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HSV Image\n",
        "Hue (H), Saturation (S) and Value (V)\n",
        "\n",
        "- Hue channel contains information related to color.\n",
        "- Saturation channel comprises of the shades of the color.\n",
        "- Value stands for the intensity of the luminance.\n",
        "\n",
        "The components of hue and saturation remain majorly indifferent to lighting conditions. The value component will change as per the lighting. Since in HSV color space, sources of chrominance and luminance are separate, it becomes easier to perform color segmentation by specifying a threshold."
      ],
      "metadata": {
        "id": "fPBXlANWnKH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting HSV Image\n",
        "image = cv2.imread('image_url')\n",
        "HSV_Image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "cv2_imshow(HSV_Image)"
      ],
      "metadata": {
        "id": "XbpI2nBSnerf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YCrCb Colorspace\n",
        "Y stands for luma (intensity of luminance), Cr represents the red component after subtracting the luma component (R - Y) and similarly, Cb represents the blue component after subtracting the luma component (B - Y).\n",
        "\n",
        "The distinct components of chroma and luminance aid in the effective separation of the colors; specifically, this color space works best for distinguishing the red and blue colors from an image."
      ],
      "metadata": {
        "id": "TvG__e1KoJE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting YCrCb Image\n",
        "image = cv2.imread('image_url')\n",
        "YCrCb_Image = cv2.cvtColor(image, cv2.COLOR_BGR2YCR_CB)\n",
        "cv2_imshow(YCrCb_Image)"
      ],
      "metadata": {
        "id": "njnE8D1NoBxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAB\n",
        "This color space also encodes the information of luminance and chroma in separate channels.\n",
        "\n",
        "- The L channel corresponds to lightness (lighting intensity).\n",
        "- The A and B components store color details, with the former consisting of color components ranging from green to magenta and the latter blue to yellow.\n",
        "\n",
        "As mentioned, the change in illumination will affect the Y component. The A and B components will significantly show the difference of color information irrespective of lighting conditions."
      ],
      "metadata": {
        "id": "Y1yg6mWioryI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting LAB Image\n",
        "image = cv2.imread('image_url')\n",
        "LAB_Image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "cv2_imshow(LAB_Image)"
      ],
      "metadata": {
        "id": "CHQfnO5No_lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blurring\n",
        "aka **smoothening**, is an essential step in any image processing application.\n",
        "\n",
        "I advice to read OpenCV documentation for the easiest explanation for this topic. [OpenCV Filtering](https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html)"
      ],
      "metadata": {
        "id": "jsxaHGFTqp5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Read in the image\n",
        "image = cv2.imread('image_url')\n",
        "\n",
        "# Make a copy of the image\n",
        "image_copy = np.copy(image)\n",
        "\n",
        "# Change color to RGB (from BGR)\n",
        "image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(image_copy)"
      ],
      "metadata": {
        "id": "-w0KPWS_wdgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to grayscale for filtering\n",
        "gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# Create a Gaussian blurred image\n",
        "gray_blur = cv2.GaussianBlur(gray, (9, 9), 0)\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "\n",
        "ax1.set_title('original gray')\n",
        "ax1.imshow(gray, cmap='gray')\n",
        "\n",
        "ax2.set_title('blurred image')\n",
        "ax2.imshow(gray_blur, cmap='gray')"
      ],
      "metadata": {
        "id": "hZcEOzb7wlF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# High-pass filter\n",
        "\n",
        "# 3x3 sobel filters for edge detection\n",
        "sobel_x = np.array([[ -1, 0, 1],\n",
        "                   [ -2, 0, 2],\n",
        "                   [ -1, 0, 1]])\n",
        "\n",
        "\n",
        "sobel_y = np.array([[ -1, -2, -1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 2, 1]])\n",
        "\n",
        "\n",
        "# Filter the orginal and blurred grayscale images using filter2D\n",
        "filtered = cv2.filter2D(gray, -1, sobel_x)\n",
        "\n",
        "filtered_blurred = cv2.filter2D(gray_blur, -1, sobel_y)\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "\n",
        "ax1.set_title('original gray')\n",
        "ax1.imshow(filtered, cmap='gray')\n",
        "\n",
        "ax2.set_title('blurred image')\n",
        "ax2.imshow(filtered_blurred, cmap='gray')"
      ],
      "metadata": {
        "id": "Qv7g48ojwlxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Sharpening\n",
        "Image sharpening is just the opposite of blurring. It emphasises the variation in the neighbouring pixels so that edges look more vivid."
      ],
      "metadata": {
        "id": "C6jAyYh6s3rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge Detection\n",
        "The detection of edges in an image enables us to identify the objects that are present. The edges are formed by a significant variation in the adjacent pixel intensities of an image.\n",
        "\n",
        "### Canny Edge Detection\n",
        "It is robust and highly efficient as it incorporates the Sobel filter method along with some post-processing steps. It involves the following steps:\n",
        "- Noise Reduction\n",
        "- Sobel Filtering\n",
        "- Non Maximum Suppression\n",
        "- Hysteresis Thresholding\n",
        "\n",
        "Read more at [https://docs.opencv.org/4.x/d7/de1/tutorial_js_canny.html](https://docs.opencv.org/4.x/d7/de1/tutorial_js_canny.html)"
      ],
      "metadata": {
        "id": "viLfcXqktbdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Read in the image\n",
        "image = cv2.imread('image_url')\n",
        "\n",
        "# Change color to RGB (from BGR)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "-Wv-ImKgr8QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "plt.imshow(gray, cmap='gray')"
      ],
      "metadata": {
        "id": "_bnlYeirwBaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wide = cv2.Canny(gray, 30, 100)\n",
        "tight = cv2.Canny(gray, 200, 240)\n",
        "\n",
        "# Display the images\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "\n",
        "ax1.set_title('wide')\n",
        "ax1.imshow(wide, cmap='gray')\n",
        "\n",
        "ax2.set_title('tight')\n",
        "ax2.imshow(tight, cmap='gray')"
      ],
      "metadata": {
        "id": "3iXRmqKpwEQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colvolutional Layer\n"
      ],
      "metadata": {
        "id": "d42GqtEFyd0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# TODO: Feel free to try out your own images here by changing img_path\n",
        "# to a file path to another image on your computer!\n",
        "img_path = '/content/udacity_sdc.png'\n",
        "\n",
        "# load color image\n",
        "bgr_img = cv2.imread(img_path)\n",
        "# convert to grayscale\n",
        "gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# normalize, rescale entries to lie in [0,1]\n",
        "gray_img = gray_img.astype(\"float32\")/255\n",
        "\n",
        "# plot image\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g9DK5_PUyf1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and visualize the filter\n",
        "import numpy as np\n",
        "\n",
        "## TODO: Feel free to modify the numbers here, to try out another filter!\n",
        "filter_vals = np.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])\n",
        "\n",
        "print('Filter shape: ', filter_vals.shape)"
      ],
      "metadata": {
        "id": "1lMQH-q9yiin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining four different filters, for the sake of simplicity\n",
        "# all of which are linear combinations of the `filter_vals` defined above\n",
        "\n",
        "# define four filters\n",
        "filter_1 = filter_vals\n",
        "filter_2 = -filter_1\n",
        "filter_3 = filter_1.T\n",
        "filter_4 = -filter_3\n",
        "filters = np.array([filter_1, filter_2, filter_3, filter_4])\n",
        "\n",
        "# For an example, print out the values of filter 1\n",
        "print('Filter 1: \\n', filter_1)"
      ],
      "metadata": {
        "id": "m1DNlPUnymdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize all four filters (keep unchanged)\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "for i in range(4):\n",
        "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(filters[i], cmap='gray')\n",
        "    ax.set_title('Filter %s' % str(i+1))\n",
        "    width, height = filters[i].shape\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            ax.annotate(str(filters[i][x][y]), xy=(y,x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center',\n",
        "                        color='white' if filters[i][x][y]<0 else 'black')"
      ],
      "metadata": {
        "id": "_4QTzUKIysG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Convolutional Layer\n",
        "# Initialize a single convolutional layer so that it contains all your created filters\n",
        "# Note that you are not training this network;\n",
        "# you are initializing the weights in a convolutional layer so that you can\n",
        "# visualize what happens after a forward pass through this network!\n",
        "# Remember: no training!\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define a neural network with a single convolutional layer with four filters\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, weight):\n",
        "        super(Net, self).__init__()\n",
        "        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters\n",
        "        k_height, k_width = weight.shape[2:]\n",
        "        # assumes there are 4 grayscale filters\n",
        "        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n",
        "        self.conv.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculates the output of a convolutional layer\n",
        "        # pre- and post-activation\n",
        "        conv_x = self.conv(x)\n",
        "        activated_x = F.relu(conv_x)\n",
        "\n",
        "        # returns both layers\n",
        "        return conv_x, activated_x\n",
        "\n",
        "# instantiate the model and set the weights\n",
        "weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\n",
        "model = Net(weight)\n",
        "\n",
        "# print out the layer in the network\n",
        "print(model)"
      ],
      "metadata": {
        "id": "nagiSjVuyvq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the output of each filter\n",
        "# First, we'll define a helper function, 'viz_layer'\n",
        "# that takes in a specific layer and number of filters (optional argument),\n",
        "# and displays the output of that layer once an image has been passed through.\n",
        "\n",
        "# helper function for visualizing the output of a given layer\n",
        "# default number of filters is 4\n",
        "def viz_layer(layer, n_filters= 4):\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "    for i in range(n_filters):\n",
        "        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])\n",
        "        # grab layer outputs\n",
        "        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap='gray')\n",
        "        ax.set_title('Output %s' % str(i+1))"
      ],
      "metadata": {
        "id": "PEx-Qjem3Sf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the output of a convolutional layer, before and after a ReLu activation function is applied.\n",
        "\n",
        "# plot original image\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "\n",
        "# visualize all filters\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\n",
        "for i in range(4):\n",
        "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(filters[i], cmap='gray')\n",
        "    ax.set_title('Filter %s' % str(i+1))\n",
        "\n",
        "\n",
        "# convert the image into an input Tensor\n",
        "gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "# get the convolutional layer (pre and post activation)\n",
        "conv_layer, activated_layer = model(gray_img_tensor)\n",
        "\n",
        "# visualize the output of a conv layer\n",
        "viz_layer(conv_layer)"
      ],
      "metadata": {
        "id": "93eejsVr_QM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after a ReLu is applied\n",
        "# visualize the output of an activated conv layer\n",
        "viz_layer(activated_layer)"
      ],
      "metadata": {
        "id": "Zaz9kRzA_VW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Visualize FashionMNIST\n",
        "In this notebook, we load and look at images from the [Fashion-MNIST database](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "The first step in any classification problem is to look at the dataset you are working with. This will give you some details about the format of images and labels, as well as some insight into how you might approach defining a network to recognize patterns in such an image set.\n",
        "\n",
        "PyTorch has some built-in datasets that you can use, and FashionMNIST is one of them; it has already been dowloaded into the `data/` directory in this notebook, so all we have to do is load these images using the FashionMNIST dataset class and load the data in batches with a `DataLoader`.\n",
        "\n",
        "## Load the [data](https://pytorch.org/docs/master/torchvision/datasets.html)\n",
        "### Dataset class and Tensors\n",
        "`torch.utils.data.Dataset` is an abstract class representing a dataset. The FashionMNIST class is an extension of this Dataset class and it allows us to 1. load batches of image/label data, and 2. uniformly apply transformations to our data, such as turning all our images into Tensor's for training a neural network. *Tensors are similar to numpy arrays, but can also be used on a GPU to accelerate computing.*\n",
        "\n",
        "Let's see how to construct a training dataset."
      ],
      "metadata": {
        "id": "gvZyycCrAXvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our basic libraries\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# data loading and transforming\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# We transform them to Tensors for input into a CNN\n",
        "\n",
        "## Define a transform to read the data in as a tensor\n",
        "data_transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = FashionMNIST(root='./data', train=True,\n",
        "                                   download=True, transform=data_transform)\n",
        "\n",
        "test_data = FashionMNIST(root='./data', train=False,\n",
        "                                  download=True, transform=data_transform)\n",
        "\n",
        "\n",
        "# Print out some stats about the training and test data\n",
        "print('Train data, number of images: ', len(train_data))\n",
        "print('Test data, number of images: ', len(test_data))"
      ],
      "metadata": {
        "id": "p3HRsLe6_83G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data iteration and batching\n",
        "Next, we'll use `torch.utils.data.DataLoader` , which is an iterator that allows us to batch and shuffle the data.\n",
        "\n",
        "In the next cell, we shuffle the data and load in image/label data in batches of size 20."
      ],
      "metadata": {
        "id": "SU42QGDnBhmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data loaders, set the batch_size\n",
        "## TODO: you can try changing the batch_size to be larger or smaller\n",
        "## when you get to training your network, see how batch_size affects the loss\n",
        "batch_size = 20\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "metadata": {
        "id": "DYY4sjp2BhR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize some training data\n",
        "This cell iterates over the training dataset, loading a random batch of image/label data, using `dataiter.next()`. It then plots the batch of images and labels in a `2 x batch_size/2` grid."
      ],
      "metadata": {
        "id": "eNigkbICBvxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, int(batch_size/2), idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "metadata": {
        "id": "GIcd_LDAB6AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View an image in more detail\n",
        "Each image in this dataset is a `28x28` pixel, normalized, grayscale image.\n",
        "\n",
        "### A note on normalization\n",
        "Normalization ensures that, as we go through a feedforward and then backpropagation step in training our CNN, that each image feature will fall within a similar range of values and not overly activate any particular layer in our network. During the feedfoward step, a network takes in an input image and multiplies each input pixel by some convolutional filter weights (and adds biases!), then it applies some activation and pooling functions. Without normalization, it's much more likely that the calculated gradients in the backpropagaton step will be quite large and cause our loss to increase instead of converge."
      ],
      "metadata": {
        "id": "sMo7fQ1ECLDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select an image by index\n",
        "idx = 2\n",
        "img = np.squeeze(images[idx])\n",
        "\n",
        "# display the pixel values in that image\n",
        "fig = plt.figure(figsize = (12,12))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')\n",
        "width, height = img.shape\n",
        "thresh = img.max()/2.5\n",
        "for x in range(width):\n",
        "    for y in range(height):\n",
        "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "        ax.annotate(str(val), xy=(y,x),\n",
        "                    horizontalalignment='center',\n",
        "                    verticalalignment='center',\n",
        "                    color='white' if img[x][y]<thresh else 'black')"
      ],
      "metadata": {
        "id": "MoTSDy7wCVOI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}